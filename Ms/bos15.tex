\documentclass[12pt]{article}
\usepackage[breaklinks=true]{hyperref}
% \usepackage[html,png]{tex4ht}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{natbib}

\newcommand{\cD}{{\mathcal D}}
\newcommand{\cF}{{\mathcal F}}
\newcommand{\todo}[1]{{\color{red}{TO DO: \sc #1}}}

\title{Student Evaluations of Teaching (Mostly) Do Not Measure Teaching Effectiveness}
\author{Anne Boring, Kellie Ottoboni, Philip B.~Stark}
\date{Draft \today}
\begin{document}
\maketitle

\newpage
\begin{quotation}
    \emph{The truth will set you free, but first it will piss you off.}
    
     \hfill Gloria Steinem

\begin{abstract}
We examine student evaluations of teaching (SET) at SciencesPo
University, Paris, where all
first-year students take the same courses 
(economics, history, political science, sociology, and political institutions). 
Students are assigned to sections of those courses as if at random, creating a natural experiment.
Final exams are set for the entire course
by the professor rather than the section instructor, and are graded anonymously.
Hence, final exam scores are a proxy for the effectiveness of the section instructors.
SET are mandatory.
We study relationships among SET and the genders of students and
instructors, topic, final exam scores, and students' grade expectations
for 22,665 SETs of 372 instructors by 4,423 students over five years.
Nonparametric permutation tests that aggregate within the 1,177 course sections show: 
\begin{itemize}
   \item the association between ratings and final 
            exam scores is negative but insignificant
            (2-sided $P \approx 0.57$)
   \item the association between instructor gender 
            and final exam scores is insignificant
            (students of male instructors do worse, 2-sided $P \approx 0.52$)
   \item the association between ratings and grade 
            expectations is positive and highly significant
            (2-sided $P \approx 0.00$)
   \item the association between instructor 
            gender and ratings is highly significant 
            (men get higher ratings, 2-sided $P \approx 0.00$)
   \item male students rate male instructors significantly higher (2-sided $P \approx 0.00$) 
            but male students score (insignificantly) lower on final exams in courses taught
            by male instructors (2-sided $P \approx 0.76$)
   \item female students rate male instructors higher, but not significantly (2-sided $P \approx 0.53$)
            but female students score (insignificantly) lower on final exams in courses taught
            by male instructors (2-sided $\approx 0.68$)
\end{itemize}
These relationships vary by discipline.
%Student responses fail simple tests of data quality.
%For instance, 29\% of students report spending impossible amounts of time
%on their courses.

\end{abstract}

\newpage

\end{quotation}

\section{Background}
Student evaluations of teaching (SET) are used widely in higher education 
as a measure of teaching quality,
and figure in the hiring, promotion, and firing of instructors, especially non-tenured faculty.
SET are generally treated as a measure of teaching effectiveness, rather than, e.g., a measure of student satisfaction.
Because ascertaining teaching effectiveness is so difficult---for students,
faculty, and administrators alike---attempts to measure teaching effectiveness by
surveying student opinion may suffer from conscious or unconscious biases. 
Recent work by \citet{mcNellEtal14} has demonstrated that this is the case: 
their randomized, controlled experiment shows that, on average, students rate a given instructor
lower on every aspect of teaching (including ``objective'' measures such as
timeliness) when they think the instructor is female than when 
they think the instructor is male.

Randomized, controlled experiments also show that SET do not measure
teaching effectiveness; the key studies are \citet{carrellWest10,bragaEtal11},
which find that students confuse grades (or grade expectations) with long-term
value.

Here, we use a remarkable census of SET by first-year students at SciencesPo (Paris)
collected between 2008 and 2013, 
comprising 22,665 SETs by
4,423 students of 1,177
sections taught by 372 instructors.
These data are discussed in detail by \citet{boring15}.
The key aspects of the data are these:
\begin{itemize}
   \item All first year students take the same six courses, in history, macroeconomics, microeconomics, 
            political institutions, political science, and sociology.
            Each course has one main professor,
            who delivers the lectures (to groups of approximately 900 students) and creates 
            the final exams.
            Courses have many sections of 10--24 students. 
            Those sections are taught by different instructors.
            The instructors have considerable pedagogical freedom.
    
   \item Students enroll in ``triads'' of sections of these courses. 
            The enrollment process
            does not allow students the freedom to select individual instructors.
            The assignment of students to sections is ``as if'' at random.
            
   \item Section instructors provide interim grades during the term. 
            Students know what their interim grades are, so interim grades are a 
            good measure of grade expectations.
            
   \item Final exams are set by the main professor: all students in a given course take the
            same final. Final exams are graded anonymously in all disciplines except Political
            Institutions (which we omit from analyses involving final exam scores).
            This makes performance on the final exam a reasonable measure of the value the
            section instructor adds: students of more effective instructors should do better on
            the final exam, on average.
    
   \item SET are mandatory: the response rates are nearly perfect.
   
\end{itemize}

SETs include closed-ended and open-ended questions, 
but the question that attracts the most attention is the overall 
score, which is considered to be a summary 
of the scores on the other questions. 

%How do you evaluate:
%\begin{itemize}
%\item the preparation and the organization of classes?    
%\item the quality of the teaching materials?
%\item the clarity of the assessment criteria?
%\item the usefulness of feedback?
%\item your teacher's class leadership skills?
%\item your teacher's ability to encourage group work? 
%\item your teacher's availability and communication skills?
%\item the course's ability to relate to current issues?
%\item your teacher's contribution to your intellectual development?
%\end{itemize}
%
%What is your overall level of satisfaction?
%
%For this set of questions, students have a choice between answering non-pertinent, insufficient, average, good or excellent.
%
%The other closed-ended questions that students answer are:
%Compared with other courses this semester, I invested much more effort / as much effort / much less effort in this course. 
%How many assessments did you have throughout the semester? 
%0 to 2 / 3 to 4 / 5 to 6 / 7 or more
%Were written assignments given back within the time deadlines?
%Were oral presentation grades given back within the time deadlines? 
%
%Finally, the SET form ends with two closed-ended questions: 
%What are the strong points of this course? 
%What are the points that the teacher could improve?     
 
We investigate 
hypotheses relating to whether the overall satisfaction score SET primarily measures teaching
effectiveness or something else, for instance, the gender of the instructor or students'
grade expectations.
The data also allow us to determine whether there are systematic differences
in how students rate courses in different disciplines.

We use nonparametric permutation tests rather than, for instance, logistic regression.
Using nonparametric tests allows us to avoid counterfactual assumptions about
generative models for the data, which regression-based methods (including
ordinary linear regression, mixed effects models, logistic regression, etc.) and parametric
methods such as $t$-tests and ANOVA, would require.
The null hypotheses for our tests are simply that some 
characteristic---e.g., instructor gender---amounts to an arbitrary label, and might as well
have been assigned at random. 

Our analysis is conducted at the level of courses, which matches how SET are
used in practice by institutions: typically, student responses in a given course
are averaged, and those averages are compared across instances of the course,
across courses in a department, and across departments within a university.
Some of the statistical issues in this reduction of SET to averages are 
discussed by \citet{starkFreishtat14}


%\section{Data}
%\subsection{SciencesPo Curriculum}
%First-year undergraduate students take six mandatory courses: in the fall,
%introduction to microeconomics, political institutions, and history; 
%and in the spring, introduction to macroeconomics, political 
%science, and sociology.
%Semesters last twelve weeks.
%Each week, for each course, students attend two one-hour lectures by a tenured professor
%(approximately 900 students per lecture) and two one-hour tutorial sections 
%(between 10 and 24 students per section). 
%Our SET data include students' individual evaluations of instructors 
%in these sections for microeconomics, history, political institutions, and 
%macroeconomics for the five academic years 2008--2013,
%and for sociology and political science courses for the three academic years 2010--2013 
%(these two were introduced in 2010).
%
%In the fall, students enroll in sections for the whole year, in cohorts called ``triads'':
%students in a triad take the same sections of all three courses.
%Students cannot pick and choose among individual instructors separately for
%each course, only among triads of sections.
%When possible according to students' and instructors' schedules, 
%students stay in the same triad for the spring semester courses. 
%Students are not allowed to change triads once courses have started in the fall. 
%
%The professor who delivers the lectures sets the content of the course and 
%writes the final exam for the course as a whole. 
%Section instructors devise their own syllabi;
%the administration encourages pedagogical freedom, especially regarding 
%the assignments that instructors use to assess student progress during each term. 
%Section instructors must, however, follow the course program set by the professor. 
%
%One way to measure teaching effectiveness is to check how students perform on the final exam. 
%The main lecturer for the course writes the final exam, which all students have to take to pass the class 
%(the final exam grade counts as one third of the final grade for the course). 
%All papers are pooled and graded anonymously by different instructors of the course, such that instructors wouldn't know 
%if they were actually grading one of their students or other students. 
%There are written exams for five out of the six 
%courses, whereas the political institutions final exam is an oral exam. 
%The instructor who gives the final exam in the 
%political institutions course is a different instructor from the one that students had during the semester;
%nonetheless, the grading is not anonymous and there is a large imbalance in the genders of the
%instructors in political institutions---52 men and 12 women--so we omit political institutions courses
%from our analysis.
%If SET scores actually measure teaching effectiveness, then students who rate instructors 
%higher would be likely to obtain higher final grades. 

%Students have been completing their SETs online since 2008. 
%The response rate is close to 100\% as it is mandatory for 
%students to complete their SETs. 
%They risk a number of sanctions if they do not complete their SETs, such as not being able 
%to register in the following semester. Students complete their SETs 
%before they take their final exams. The SET scores are
%anonymous to the teachers, who only have access to them once all grades have been officially recorded on student transcripts, 
%several weeks after final exams. Instructors and academic coordinators then have access to SETs. When scores are low, the 
%academic coordinator discusses the SETs with the instructor.   

\section{Tests}

\section{Code}
Github repo. \url{https://github.com/kellieotto/SET-and-Gender-Bias}

\section{Discussion}
Implicit in the use of of SET as a 
Push back on the notion of ``teaching effectiveness.''
There ought to be \emph{some} interaction between characteristics of the
instructor and those of the student.
If ``effectiveness'' is intrinsic to the instructor, ratings in one class shouldn't depend on
which other classes a student takes.
Looking at ratings ``per student'' doesn't make sense if you are trying to
measure some underlying platonic ``effectiveness'' intrinsic to the instructor.
In particular,  a showing that individual students who give a particular instructor higher ratings
get higher grades, does not point to ....\todo{fix me}

Mention defenses of SET?  The correlation between SET and performance isn't zero:
it is positive, albeit not statistically significant.
The larger point is that SET are better measures of student grade expectations and
of instructor gender than they are of teaching effectiveness.

\section{Conclusions}

\end{document}
