\documentclass[12pt]{article}
\usepackage[breaklinks=true]{hyperref}
% \usepackage[html,png]{tex4ht}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}

\newcommand{\cD}{{\mathcal D}}
\newcommand{\cF}{{\mathcal F}}
\newcommand{\todo}[1]{{\color{red}{TO DO: \sc #1}}}

\title{What do Students Mean by ``Teaching Effectiveness"?}
\author{Anne Boring, Kellie Ottoboni, Philip B.~Stark}
\date{Draft \today}
\begin{document}
\maketitle

\section{Background}
Push back on the notion of ``teaching effectiveness.''
There ought to be \emph{some} interaction between characteristics of the
instructor and those of the student.
If ``effectiveness'' is intrinsic to the instructor, ratings in one class shouldn't depend on
which other classes a student takes.
Looking at ratings ``per student'' doesn't make sense if you are trying to
measure some underlying platonic ``effectiveness'' intrinsic to the instructor.
In particular,  a showing that individual students who give a particular instructor higher ratings
get higher grades, does not point to ....\todo{fix me}

Carrell \& West, Braga, Paccagnella, \& Pellizzari, 2011 on validity.
Cite McNell, Boring on gender.
Cite Lauer on comments.
Cite defenders IDEA Benton \& Cashin in defense of SET.  
Cite Spooren, Brockx, and Mortelmans 2013 for discussion on reliability and validity. Apart for student performance on 
the final exam, they cite an alternative to test for convergent validity (correlation between measures of student learning 
and SETs): using the class mean (as suggested by Clayson 2009) 

How do these things fit together?

Reliability and validity: the correlation argument.

\section{Data}
\subsection{Organization of courses}

First year undergraduate students at this university are required to take six mandatory courses: introduction to 
microeconomics, political institutions and history during the fall semester; and introduction to macroeconomics, political 
science and sociology during the spring semester. Each semester lasts twelve weeks, during which 
students follow two hour long lectures by a tenured professor
(approximately 900 students per lecture), and two hour long smaller tutoring classes for each course 
(between 10 and 24 students per tutoring class). The database includes students' individual evaluations of instructors 
in these small classes for microeconomics, history, political institutions and macroeconomics for five academic years from 
2008-2009 to 2012-2013. The database also includes students' individual evaluations of teachers for the sociology and
political science courses for three academic years, as these two courses were introduced in 2010-2011.

The small classes are organized in ``triads''. Each student registers for one triad at the beginning of the fall semester, 
which means that a student be with the same group of students in the tutoring sessions for history, microeconomics and 
political institutions. When possible according to students' and instructors' schedules, students then stay in the same triad
for the spring semester courses. Students are not allowed to change triads once courses have started during the fall semester. 

The main lecturer decides on the content of the course and writes the final exam for the course. Instructors write-up their 
own syllabi and the administration values and encourages pedagogical freedom for individual instructors, especially regarding 
the assignments that they use in the continuous assessment grades. Instructors must, however, follow the course program which
is set by the main lecturer. 

One way to measure teaching effectiveness is to check how students perform on the final exam. 
The main lecturer for the course writes the final exam, which all students have to take to pass the class 
(the final exam grade counts as one third of the final grade for the course). 
All papers are pooled and graded anonymously by different instructors of the course, such that instructors wouldn't know 
if they were actually grading one of their students or other students. There are written exams for five out of the six 
courses, whereas the political institutions final exam is an oral exam. The instructor who gives the final exam in the 
political institutions course is a different instructor from the one that students had during the semester. 
If SET scores actually measure teaching effectiveness, then students who rate instructors higher would be likely to obtain
higher final grades. 

Students have been completing their SETs online since 2008. The response rate is close to 100\% as it is mandatory for 
students to complete their SETs. They risk a number of sanctions if they do not complete their SETs, such as not being able 
to register in the following semester. Students complete their SETs before they take their final exams. The SET scores are
anonymous to the teachers, who only have access to them once all grades have been officially recorded on student transcripts, 
several weeks after final exams. Instructors and academic coordinators then have access to SETs. When scores are low, the 
academic coordinator discusses the SETs with the instructor.   

SETs include closed-ended and open-ended questions, but the question which attracts the most attention is the overall 
satisfaction score, which is considered to be a summary of the scores on the other questions. The complete set of questions 
that students fill-out is as follows:

How do you evaluate:
\begin{itemize}
\item the preparation and the organization of classes?    
\item the quality of the teaching materials?
\item the clarity of the assessment criteria?
\item the usefulness of feedback?
\item your teacher's class leadership skills?
\item your teacher's ability to encourage group work? 
\item your teacher's availability and communication skills?
\item the course's ability to relate to current issues?
\item your teacher's contribution to your intellectual development?
\end{itemize}

What is your overall level of satisfaction?

For this set of questions, students have a choice between answering non-pertinent, insufficient, average, good or excellent.

The other closed-ended questions that students answer are:
Compared with other courses this semester, I invested much more effort / as much effort / much less effort in this course. 
How many assessments did you have throughout the semester? 0 to 2 / 3 to 4 / 5 to 6 / 7 or more
Were written assignments given back within the time deadlines?
Were oral presentation grades given back within the time deadlines? 

Finally, the SET form ends with two closed-ended questions: 
What are the strong points of this course? 
What are the points that the teacher could improve?      


We need to remove or run separate analyses for affirmative action (CEP) students. They tend to get lower overall 
grades than the rest of the students, which may be a confounder.

\section{Tests}

\subsection{Per instructor}
Pearson correlation between a summary statistic of effectiveness rating and a summary statistic
of student performance, e.g., mean effectiveness (on various dimensions) and pass rate or mean
final exam score.
\todo{code is ready.  Do we do all metrics?}

\subsubsection{Gender}
Pearson correlation between a summary statistic of effectiveness rating and gender of instructor
\todo{code is ready.  Do we do all metrics?}

\subsection{Per student}
For a single student, test correlation between course rating (overall, individual dimensions) and final grade/interim grade/professor gender.
The null hypothesis is no correlation between rating and $x$.

The test assumes independence among students within a triad of classes.
First null: all $3! = 6$ orderings equally likely.
If we don't reject that, no need to go further.

We \emph{could} allow for unequal probability that students have preferences
related, e.g., to grades, but keep the independence assumption.
One candidate weighting scheme is:

$$P(\text{teacher $i$ gets rated best}) = \frac{\%\text{ students given CAS }> t\text{ by teacher $i$}}{\sum_{j=1}^3(\%\text{ students given CAS }> t\text{ by teacher }j)}$$
 
Or more simply, 

$$P(\text{teacher $i$ gets rated best}) = \frac{\text{CAS from teacher $i$}}{\sum_{j=1}^3\text{ CAS from teacher }j}$$
Aggregate the test statistics (Pearson correlation) across strata (different students) to get an overall p-value.  There are roughly $(3!)^{14}$ different possible permutations.
We expect to reject the null for the interim grades and for gender; do not expect to reject the null for final grades.
 
Confidence bounds:
We can lower bound the ``female disadvantage,'' i.e., 
how much a female teacher needs to improve her ratings in order to destroy the 
significant association from the test, assuming a constant effect size.
Also can lower bound how much a teacher can lower the interim (continuous assessment) grades to break the association between interim grades and SET

\section{Inter-rater reliability}
There is a distinction between teaching evaluations measuring something unique to each student 
(value added for them, from a particular teacher) versus 
measuring something intrinsic about the teacher.  
The goal of teaching evaluations is to measure intrinsic teaching ability.  
How well this is accomplished should be reflected in how similarly 
students rate their 6 teachers.
We can rank the 6 teachers for each student based on the ratings 
they've assigned, then measure concordance between students in a triad by asking how often they ranked teacher i with rank j.
Other measures of value added on the instructor level include the fraction of 
students who pass or the fraction of students with a final grade above x.  
On this line of reasoning, we can do a permutation test for the 
Pearson correlation between median rating from students in a class and the pass rate.
Issues:
We'll want to do these analyses separately for male and female students, 
since there seems to be an interaction effect between student gender and teacher gender.
We assume stationarity: students will be the same and perform the same from semester to semester.

\subsection{Gender effects}
Look at the interaction of grades and gender: 
do students require higher grades from female teachers for warm glow effect?
Triplet effect for gender--pool triplets with same number of instructors of a given gender. \\

Another potential idea for controlling is to match. One approach is to use students as their own control, looking at a pair of classes in which they got the same grade, one taught by a male and the other by a female. An approach that will be more powerful and easier to implement is matching/binning students on their final exam scores/overall class grades; within matches or bins, we can do a sort of sign test to compare the mean ratings of male and female instructors. Within a bin, it's like a coin flip to decide whether the average male or female rating is higher - test if the coin is fair or biased. Preliminary results suggest that the effect is more pronounced when comparing excellent to good ratings.

\subsection{Relative or absolute}
Hypothesis: students are comparing teachers rather than making absolute judgments.


\subsection{Punishment}
Look at students who took two courses from the same instructor, and got a lower course grade than the interim grade in the first course.
Null hypothesis: equally likely to rate the instructor higher in the first and second course, 
independent of each other.
Alternative: more likely to ``punish'' the instructor with a bad rating in the second course.


\section{Code}
Github repo.

\section{Discussion}

\section{Conclusions}

\end{document}
